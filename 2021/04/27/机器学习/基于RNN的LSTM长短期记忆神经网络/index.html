<!DOCTYPE html>
<html lang="en">
  <head><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="description" content="基于RNN的LSTM长短期记忆神经网络"/><meta name="keywords" content="keywords" /><link rel="alternate" href="https://github.com/paper00/paper00.github.io/issues" title="Euphoria"><link rel="shortcut icon" type="image/x-icon" href="/cookie128px.ico?v=2.11.0" />
<link rel="canonical" href="https://paper00.github.io/2021/04/27/机器学习/基于RNN的LSTM长短期记忆神经网络/"/>

<link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css" />
<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.11.0" />

<script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
<script>
  window.config = {"leancloud":{"app_id":null,"app_key":null},"toc":true,"fancybox":true,"pjax":"","latex":false};
</script>

    <title>基于RNN的LSTM长短期记忆神经网络 - Euphoria</title>
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/." class="logo">Euphoria</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list"><a href="/">
        <li class="mobile-menu-item">Home
          </li>
      </a><a href="/archives/">
        <li class="mobile-menu-item">Archives
          </li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags
          </li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories
          </li>
      </a></ul>
</nav>
<div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Euphoria</a>
</div>

<nav class="site-navbar"><ul id="menu" class="menu"><li class="menu-item">
          <a class="menu-item-link" href="/">
            Home
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            Archives
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/tags/">
            Tags
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/categories/">
            Categories
            </a>
        </li>
      </ul></nav>
</header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content"><article class="post">
    <header class="post-header">
      <h1 class="post-title">基于RNN的LSTM长短期记忆神经网络
        </h1>

      <div class="post-meta">
        <span class="post-time">
          2021-04-27
        </span><span class="post-category">
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
            </span>
        </div>
    </header>

    <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">Contents</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN-神经网络"><span class="toc-text">RNN 神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM-神经网络"><span class="toc-text">LSTM 神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#遗忘门"><span class="toc-text">遗忘门</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#输入门"><span class="toc-text">输入门</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#new-value"><span class="toc-text">new value</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#更新传输带Ct"><span class="toc-text">更新传输带Ct</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#输出门"><span class="toc-text">输出门</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#更新状态向量-h-t"><span class="toc-text">更新状态向量 h_t</span></a></li></ol></li></ol>
    </div>
  </div><div class="post-content"><h2 id="RNN-神经网络"><a href="#RNN-神经网络" class="headerlink" title="RNN 神经网络"></a>RNN 神经网络</h2><p><code>RNN</code>(Recurrent Neural Network, 循环神经网络) 的目的是用来处理序列数据。在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，而每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题却无能无力。例如，你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。RNNs之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的 <strong><em>节点</em></strong> 不再无连接而是 <strong><em>有连接的</em></strong>，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。</p>
<p>其展开图如下：</p>
<p><img src="/images/RNN%E5%B1%95%E5%BC%80%E5%9B%BE.png" alt="RNN展开图"></p>
<p>其中，<em>h1</em> 状态包含着 <em>x0</em> 和 <em>x1</em> 的信息，<em>h1</em> 状态包含着 <em>x0</em>, <em>x1</em>, <em>x2</em> 的信息，以此类推，<em>ht</em> 里包含着所有的输入信息, 可以把 <em>ht</em> 看作为从 <em>x0</em>, <em>x1</em> ··· <em>xt</em> 整个输入信息里提取到的特征向量。</p>
<p><img src="/images/RNN%E5%8D%95%E5%B1%82%E5%9B%BE.png" alt="RNN单层图"></p>
<p>更新每个状态 <em>h</em> 的时候需要用到参数矩阵 <em>A</em>, 需要注意的是整个<code>RNN</code>只有一个参数矩阵 <em>A</em>，一开始 <em>A</em> 随机初始化，然后利用训练数据来学习 <em>A</em>. 但在此过程中，如果矩阵向量 <em>A</em> 的特征值大于 <em>1</em>，或者小于 <em>1</em>，并且循环次数很大的情况下，可能会变成接近于 <em>0</em>，或者变成很大的值。从而导致 <strong><em>梯度消失</em></strong> 和 <strong><em>梯度爆炸</em></strong>。</p>
<h2 id="LSTM-神经网络"><a href="#LSTM-神经网络" class="headerlink" title="LSTM 神经网络"></a>LSTM 神经网络</h2><p><code>LSTM</code>(Long Short Term Memory, 长短时记忆网络) 模型，是在<code>RNN</code>模型的基础上作出改进，解决了<code>RNN</code>短期记忆的问题 (梯度消失和梯度爆炸)。</p>
<p><img src="/images/LSTM%E5%9B%BE.png" alt="LSTM图"></p>
<p><code>LSTM</code>在<code>RNN</code>的基础结构上增加了遗忘门、输入门、输出门三个单元，它有四种参数矩阵。</p>
<h3 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a>遗忘门</h3><p><img src="/images/%E9%81%97%E5%BF%98%E9%97%A8.png" alt="遗忘门"><br>求<code>遗忘门</code><strong><em>ft</em></strong> 的方法 ： 先把上一时刻的 <strong><em>h_t-1</em></strong> 与当前输入 <strong><em>xt</em></strong> 连接，然后与遗忘权重矩阵 <strong><em>Wf</em></strong> 相乘，结果做 <em>sigmoid</em>。</p>
<p><strong><em>ft</em></strong> 的每一个值都在 <em>0-1</em> 之间，比如为0时，就是要忘记，为1时，就是全部通过。所以遗忘门 <strong><em>f</em></strong> 可以有选择地让传输带 <strong><em>C</em></strong> 的值通过。即<code>C * f = output</code></p>
<h3 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a>输入门</h3><p><img src="/images/%E8%BE%93%E5%85%A5%E9%97%A8.png" alt="输入门"><br>求<code>输入门</code><strong><em>ft</em></strong> 的方法 ：和<code>遗忘门</code>类似，把上一时刻的 <strong><em>h_t-1</em></strong> 与当前输入 <strong><em>xt</em></strong> 连接，然后与输入权重矩阵 <strong><em>Wi</em></strong> 相乘，再通过 <em>sigmoid</em> 函数得到 i_t.</p>
<h3 id="new-value"><a href="#new-value" class="headerlink" title="new value"></a>new value</h3><p>![new value](/Users/paper/Documents/paper/hexo-blog/source/images/new value.png)<br>求<code>new value</code><strong><em>_Ct</em></strong> 的方法 ：与<code>遗忘门</code>和<code>输入门</code>类似，但是使用的激励函数不一样，一般使用<code>tanh</code>函数，所以向量的每一个值都在 <em>-1~1</em> 之间。计算 <strong><em>_Ct</em></strong> 也需要一个权重矩阵 <strong><em>Wc</em></strong>.</p>
<h3 id="更新传输带Ct"><a href="#更新传输带Ct" class="headerlink" title="更新传输带Ct"></a>更新传输带<em>Ct</em></h3><p><img src="/images/%E6%9B%B4%E6%96%B0%E4%BC%A0%E8%BE%93%E5%B8%A6Ct%E7%9A%84%E5%80%BC.png" alt="更新传输带Ct的值"><br>我们已经求出<code>遗忘门</code><strong><em>ft</em></strong>，<code>输入门</code><strong><em>ft</em></strong>，还有<code>new value</code><strong><em>_Ct</em></strong>，我们还知道传输带旧的值 <strong><em>C_t-1</em></strong>，现在可以更新传输带 <strong><em>Ct</em></strong> 了。</p>
<p>更新<code>传输带 Ct</code>的方法（如上图）：</p>
<ol>
<li><p><code>遗忘门</code><strong><em>ft</em></strong> 选择性地遗忘旧传输带 <strong><em>Ct-1</em></strong> 中的一些元素。</p>
<blockquote>
<p><code>ft * Ct-1</code> (1)</p>
</blockquote>
</li>
<li><p>现在要往传输带上添加一些新的信息。也就是<code>输入门</code><strong><em>ft</em></strong> 向量与<code>new value</code><strong><em>_Ct</em></strong> 向量相乘。</p>
<blockquote>
<p><code>i_t * _Ct</code> (2)</p>
</blockquote>
</li>
<li><p>新的传输带 <strong><em>Ct</em></strong> 值就是上一时刻的值 <strong><em>Ct-1</em></strong> 通过遗忘门删除一些值，通过<code>输入门</code>添加一些新值得到的，也就是 <em>(1) + (2)</em>。</p>
<blockquote>
<p><code>Ct = ft * Ct-1 + i_t * _Ct</code></p>
</blockquote>
</li>
</ol>
<h3 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h3><p><img src="/images/%E8%BE%93%E5%87%BA%E9%97%A8.png" alt="输出门"><br>与<code>遗忘门</code>和<code>输入门</code>算法一样。输出门也有自己的参数矩阵 <strong><em>Wo</em></strong>.</p>
<h3 id="更新状态向量-h-t"><a href="#更新状态向量-h-t" class="headerlink" title="更新状态向量 h_t"></a>更新状态向量 h_t</h3><p><img src="/images/%E6%9B%B4%E6%96%B0%E7%8A%B6%E6%80%81%E5%90%91%E9%87%8Fh_t.png" alt="更新状态向量h_t"><br>求出 <strong><em>h_t</em></strong>的方法 ：</p>
<ol>
<li>对<code>传输带 Ct</code>每一个元素求<code>tanh</code>双曲正切，把元素都压到 <em>-1~1</em> 区间。</li>
<li><code>ht = Ot * tanh(Ct)</code></li>
</ol>
<p>这样，可以认为所有输入的 <em>x</em> 信息都积累在状态向量 <strong><em>ht</em></strong> 里面。</p>

      </div>
      
      <footer class="post-footer">
        <div class="post-tags">
            <a href="/tags/RNN/">RNN</a>
            <a href="/tags/LSTM/">LSTM</a>
            <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a>
            </div>
        
        <nav class="post-nav"><a class="next" href="/2021/04/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%B0%8F%E6%B3%A2%E5%88%86%E6%9E%90%E5%85%B7%E4%BD%93%E7%90%86%E8%A7%A3/">
        <span class="next-text nav-default">小波分析具体理解</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    </nav></footer>
    </article></div><div class="comments" id="comments"></div></div>
      </main>

      <footer id="footer" class="footer"><div class="social-links"><a href="mailto:yunduanling@naver.com" target="_blank" rel="noopener" class="iconfont icon-email" title="email"></a>
        <a href="https://github.com/paper00" target="_blank" rel="noopener" class="iconfont icon-github" title="github"></a>
        <a href="https://github.com/paper00/paper00.github.io/issues" target="_blank" rel="noopener" class="iconfont icon-rss" title="rss"></a>
    </div><div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even" target="_blank" rel="noopener">Even</a>
  </span>

  <span class="copyright-year">&copy;2018 - 2021<span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">paper</span>
  </span>
</div>
</footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div><script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/src/even.js?v=2.11.0"></script>
</body>
</html>
